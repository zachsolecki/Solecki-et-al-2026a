{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import scipy as sc\n",
    "import os\n",
    "import xarray as xr\n",
    "import tqdm\n",
    "from tqdm import tqdm, trange\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides code to prepare a matrix for the Kmeans clustering algorithm and perform the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_means = xr.open_dataarray('/path/to/temperature_data.nc') # this is the daily mean temperature data\n",
    "cleaned_clusters03 = np.load('/path/to/masked_real_clean_3.npy') # this is the clearned up CAO binary array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temperature = all_means.groupby('time.dayofyear').mean('time')\n",
    "anomalies = all_means - mean_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_array, num_features = ndimage.label(cleaned_clusters03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create arrays to store the centroids\n",
    "start_centroids = np.zeros((num_features, 2))  # (y, z) for the first slice\n",
    "end_centroids = np.zeros((num_features, 2))    # (y, z) for the last slice\n",
    "\n",
    "# Create an array to store the distances\n",
    "horizontal_distances = np.zeros(num_features)\n",
    "\n",
    "# Iterate through each labeled feature\n",
    "for label in tqdm(range(1, num_features + 1)):  # Start from 1 as 0 is background label\n",
    "    # Find the indices where the labeled feature is present in the first slice\n",
    "    indices_start = np.where(labeled_array == label)\n",
    "    first_slice_index = np.min(indices_start[0])\n",
    "    first_slice = labeled_array[first_slice_index]\n",
    "    start_ys_zs = np.where(first_slice == label)\n",
    "\n",
    "    # Find the centroid in the first slice\n",
    "    start_centroids[label - 1] = [np.mean(start_ys_zs[0]), np.mean(start_ys_zs[1])]\n",
    "\n",
    "    # Find the indices where the labeled feature is present in the last slice\n",
    "    indices_end = np.where(labeled_array == label)\n",
    "    last_slice_index = np.max(indices_end[0])\n",
    "    last_slice = labeled_array[last_slice_index]\n",
    "    end_ys_zs = np.where(last_slice == label)\n",
    "\n",
    "    # Find the centroid in the last slice\n",
    "    end_centroids[label - 1] = [np.mean(end_ys_zs[0]), np.mean(end_ys_zs[1])]\n",
    "\n",
    "    # Calculate the horizontal distance between the starting and ending centroids for each feature\n",
    "    horizontal_distances[label - 1] = np.linalg.norm(start_centroids[label - 1] - end_centroids[label - 1])\n",
    "\n",
    "# Now, 'horizontal_distances' contains the horizontal distances between the starting and ending centroids for each labeled feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_start = np.where(labeled_array == 32)\n",
    "first_slice_index = np.min(indices_start[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_x = start_centroids[:, 0]\n",
    "start_y = start_centroids[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "\n",
    "for label in tqdm(range(1, num_features + 1)):\n",
    "    indices = np.where(labeled_array == label)\n",
    "    if indices[0].size > 0:\n",
    "        depth = np.max(indices[0]) - np.min(indices[0]) + 1\n",
    "        depths.append(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized anomalies\n",
    "import numpy as np\n",
    "\n",
    "std_temps = np.std(anomalies, axis=0)\n",
    "mean = np.mean(anomalies, axis=0)\n",
    "\n",
    "normalized_anomalies = (anomalies - mean) / std_temps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "average_normalized_anomalies = np.zeros(num_features)\n",
    "centroid_max_intensity = np.zeros((num_features, 2))\n",
    "max_extent2 = []\n",
    "# Iterate through each labeled feature\n",
    "for label in tqdm(range(1, num_features + 1)):  # Start from 1 as 0 is background label\n",
    "    # Find the indices where the labeled feature is present\n",
    "    indices = np.where(labeled_array == label)\n",
    "    extent = []\n",
    "    for i in np.unique(indices[0]):\n",
    "        extent.append(np.sum(labeled_array[i]) / label)\n",
    "        max_extent = np.argmax(extent)\n",
    "        max_extent_i = max_extent + np.unique(indices[0])[0]\n",
    "    max_extent2.append(np.max(extent))\n",
    "    intense_slice = labeled_array[max_extent_i]\n",
    "    ys_zs = np.where(intense_slice == label)\n",
    "    centroid_max_intensity[label - 1] = [np.mean(ys_zs[0]), np.mean(ys_zs[1])]\n",
    "\n",
    "    # Extract the normalized anomalies for the labeled feature using the unique indices\n",
    "    for i in range(len(ys_zs[0])):\n",
    "        anomalies_for_feature = []\n",
    "        anomalies_for_feature.append(anomalies[max_extent_i][ys_zs[0][i]][ys_zs[1][i]])\n",
    "\n",
    "\n",
    "    # Calculate the average normalized anomaly for the labeled feature\n",
    "    average_normalized_anomaly = np.mean(anomalies_for_feature)\n",
    "\n",
    "    # Store the result in the array\n",
    "    average_normalized_anomalies[label - 1] = average_normalized_anomaly\n",
    "\n",
    "# Now, 'average_normalized_anomalies' contains the average normalized anomalies for each labeled feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_temps = np.mean(average_normalized_anomalies)\n",
    "std_temps = np.std(average_normalized_anomalies)\n",
    "normalized_anomalies = (average_normalized_anomalies - mean_temps) / std_temps\n",
    "\n",
    "mean_extent = np.mean(max_extent2)\n",
    "std_extent = np.std(max_extent2)\n",
    "normalized_extent = (max_extent2 - mean_extent) / std_extent\n",
    "\n",
    "mean_distances = np.mean(horizontal_distances)\n",
    "std_distances = np.std(horizontal_distances)\n",
    "normalized_distances = (horizontal_distances - mean_distances) / std_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/glade/work/zsolecki/arrays/normalized_anomalies.npy', normalized_anomalies)\n",
    "np.save('/glade/work/zsolecki/arrays/normalized_distances.npy', normalized_distances)\n",
    "np.save('/glade/work/zsolecki/arrays/normalized_extent.npy', normalized_extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_x = centroid_max_intensity[:, 0]\n",
    "centroid_y = centroid_max_intensity[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = np.mean(centroid_x)\n",
    "std_x = np.std(centroid_x)\n",
    "normalized_x = (centroid_x - mean_x) / std_x\n",
    "\n",
    "mean_y = np.mean(centroid_y)\n",
    "std_y = np.std(centroid_y)\n",
    "normalized_y = (centroid_y - mean_y) / std_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/glade/work/zsolecki/CAO_project/arrays/normalized_x_new.npy', centroid_x)\n",
    "np.save('/glade/work/zsolecki/CAO_project/arrays/notmalized_y_new.npy', centroid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = horizontal_distances / depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_speed = np.mean(speed)\n",
    "std_speed = np.std(speed)\n",
    "normalized_speed = (speed - mean_speed) / std_speed\n",
    "np.save('/glade/work/zsolecki/arrays/normalized_speed.npy', normalized_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = []\n",
    "for label in tqdm(range(1, num_features + 1)):\n",
    "    volume = np.sum(labeled_array == label)\n",
    "    volumes.append(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_volumes = np.mean(volumes)\n",
    "std_volumes = np.std(volumes)\n",
    "normalized_volumes = (volumes - mean_volumes) / std_volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/glade/work/zsolecki/arrays/normalized_volumes.npy', normalized_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = np.column_stack((\n",
    "    normalized_volumes,\n",
    "    normalized_distances,\n",
    "    normalized_extent,\n",
    "    normalized_anomalies,\n",
    "    normalized_speed,\n",
    "    normalized_x,\n",
    "    normalized_y\n",
    "))\n",
    "\n",
    "np.save('../data/feature_matrix.npy', feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "optimal_k = 5  # Replace with your optimal number of clusters\n",
    "\n",
    "# Initialize KMeans with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "\n",
    "# Fit KMeans to your feature matrix\n",
    "cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "\n",
    "# Add the cluster labels as a new column in your feature matrix\n",
    "feature_matrix_with_clusters = np.column_stack((feature_matrix, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = feature_matrix_with_clusters[:, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_0 = np.where(clusters == 0)[0]\n",
    "indices_1 = np.where(clusters == 1)[0]\n",
    "indices_2 = np.where(clusters == 2)[0]\n",
    "indices_3 = np.where(clusters == 3)[0]\n",
    "indices_4 = np.where(clusters == 4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_0 = np.array(volumes)[indices_0]\n",
    "volume_1 = np.array(volumes)[indices_1]\n",
    "volume_2 = np.array(volumes)[indices_2]\n",
    "volume_3 = np.array(volumes)[indices_3]\n",
    "volume_4 = np.array(volumes)[indices_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extent_0 = np.array(max_extent2)[indices_0]\n",
    "extent_1 = np.array(max_extent2)[indices_1]\n",
    "extent_2 = np.array(max_extent2)[indices_2]\n",
    "extent_3 = np.array(max_extent2)[indices_3]\n",
    "extent_4 = np.array(max_extent2)[indices_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_0 = np.array(speed)[indices_0]\n",
    "speed_1 = np.array(speed)[indices_1]\n",
    "speed_2 = np.array(speed)[indices_2]\n",
    "speed_3 = np.array(speed)[indices_3]\n",
    "speed_4 = np.array(speed)[indices_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_0 = np.array(average_normalized_anomalies)[indices_0]\n",
    "anomalies_1 = np.array(average_normalized_anomalies)[indices_1]\n",
    "anomalies_2 = np.array(average_normalized_anomalies)[indices_2]\n",
    "anomalies_3 = np.array(average_normalized_anomalies)[indices_3]\n",
    "anomalies_4 = np.array(average_normalized_anomalies)[indices_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_0 = np.array(horizontal_distances)[indices_0]\n",
    "distances_1 = np.array(horizontal_distances)[indices_1]\n",
    "distances_2 = np.array(horizontal_distances)[indices_2]\n",
    "distances_3 = np.array(horizontal_distances)[indices_3]\n",
    "distances_4 = np.array(horizontal_distances)[indices_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = np.array(centroid_x)[indices_0]\n",
    "x_1 = np.array(centroid_x)[indices_1]\n",
    "x_2 = np.array(centroid_x)[indices_2]\n",
    "x_3 = np.array(centroid_x)[indices_3]\n",
    "x_4 = np.array(centroid_x)[indices_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = np.array(centroid_y)[indices_0]\n",
    "y_1 = np.array(centroid_y)[indices_1]\n",
    "y_2 = np.array(centroid_y)[indices_2]\n",
    "y_3 = np.array(centroid_y)[indices_3]\n",
    "y_4 = np.array(centroid_y)[indices_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_0 = np.column_stack((\n",
    "    volume_0,\n",
    "    distances_0,\n",
    "    extent_0,\n",
    "    anomalies_0,\n",
    "    speed_0,\n",
    "    x_0,\n",
    "    y_0, \n",
    "    indices_0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_1 = np.column_stack((\n",
    "    volume_1,\n",
    "    distances_1,\n",
    "    extent_1,\n",
    "    anomalies_1,\n",
    "    speed_1,\n",
    "    x_1,\n",
    "    y_1,\n",
    "    indices_1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_2 = np.column_stack((\n",
    "    volume_2,\n",
    "    distances_2,\n",
    "    extent_2,\n",
    "    anomalies_2,\n",
    "    speed_2,\n",
    "    x_2,\n",
    "    y_2,\n",
    "    indices_2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_3 = np.column_stack((\n",
    "    volume_3,\n",
    "    distances_3,\n",
    "    extent_3,\n",
    "    anomalies_3,\n",
    "    speed_3,\n",
    "    x_3,\n",
    "    y_3,\n",
    "    indices_3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_4 = np.column_stack((\n",
    "    volume_4,\n",
    "    distances_4,\n",
    "    extent_4,\n",
    "    anomalies_4,\n",
    "    speed_4,\n",
    "    x_4,\n",
    "    y_4,\n",
    "    indices_4\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/glade/work/zsolecki/arrays/matrix_0.npy', matrix_0)\n",
    "np.save('/glade/work/zsolecki/arrays/matrix_1.npy', matrix_1)\n",
    "np.save('/glade/work/zsolecki/arrays/matrix_2.npy', matrix_2)\n",
    "np.save('/glade/work/zsolecki/arrays/matrix_3.npy', matrix_3)\n",
    "np.save('/glade/work/zsolecki/arrays/matrix_4.npy', matrix_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-research]",
   "language": "python",
   "name": "conda-env-miniconda3-research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
